{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-aAjy9Nxaoh"
      },
      "source": [
        "## Download pretrained model from huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiOaoPKaxUBS",
        "outputId": "ed791ad3-af9a-408d-ccf9-f211446dc742"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/facebook/sapiens-pretrain-0.3b/resolve/main/sapiens_0.3b_epoch_1600_clean.pth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf_V3ECgxiLv"
      },
      "source": [
        "## Define model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m9IKIfcxxhNV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=1024, patch_size=16, in_chans=3, embed_dim=1024, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = img_size // patch_size\n",
        "        self.num_patches = self.grid_size * self.grid_size\n",
        "        self.projection = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)  # (B, embed_dim, H/P, W/P)\n",
        "        x = x.flatten(2)  # (B, embed_dim, N)\n",
        "        x = x.transpose(1, 2)  # (B, N, embed_dim)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attn_drop = nn.Dropout(dropout_rate)\n",
        "        self.proj_drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                nn.Linear(in_dim, hidden_dim)  # This will be layers.0.0\n",
        "            ]),\n",
        "            nn.Linear(hidden_dim, in_dim)      # This will be layers.1\n",
        "        ])\n",
        "        self.act = nn.GELU()\n",
        "        self.drop1 = nn.Dropout(dropout_rate)\n",
        "        self.drop2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers[0][0](x)  # Apply first linear layer (layers.0.0)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.layers[1](x)     # Apply second linear layer (layers.1)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = Attention(embed_dim, num_heads, dropout_rate)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.ffn = FFN(embed_dim, hidden_dim, dropout_rate)\n",
        "        self.drop_path = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.ln1(x)))\n",
        "        x = x + self.drop_path(self.ffn(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class SapiensEncoder(nn.Module):\n",
        "    def __init__(self, img_size=1024, patch_size=16, in_chans=3, embed_dim=1024, depth=24, num_heads=16, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim, dropout_rate)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.num_patches, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, 4.0, dropout_rate) for _ in range(depth)\n",
        "        ])\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for block in self.layers:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln1(x)\n",
        "        return x\n",
        "\n",
        "def vit_base_patch16_1024(dropout_rate=0.1):\n",
        "    model = SapiensEncoder(\n",
        "        img_size=1024,\n",
        "        patch_size=16,\n",
        "        in_chans=3,\n",
        "        embed_dim=1024,\n",
        "        depth=24,\n",
        "        num_heads=16,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Example of setting a custom dropout rate\n",
        "model = vit_base_patch16_1024(dropout_rate=0.2)  # Set dropout to 0.2 for the entire model\n",
        "\n",
        "# To check all parameters and their shapes\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"{name}: {param.shape}\")\n",
        "\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "# Load the state dictionary\n",
        "state_dict = torch.load(\"sapiens_0.3b_epoch_1600_clean.pth\")\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "device = \"cuda\"\n",
        "model.to(device)\n",
        "inputs = torch.ones(1, 3, 1024, 1024).to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "    print(outputs.shape)\n",
        "    print(outputs)\n",
        "    \n",
        "    import torch\n",
        "device = \"cuda\"\n",
        "torch.cuda.empty_cache()\n",
        "ts_model = torch.jit.load(\"sapiens_0.3b_epoch_1600_torchscript.pt2\")\n",
        "ts_model.to(device)\n",
        "inputs = torch.ones(1, 3, 1024, 1024).to(device)\n",
        "ts_output = ts_model(inputs)\n",
        "ts_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkMlOdMtxlOQ"
      },
      "source": [
        "## Load pretrained Weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L13wW7Kxna2",
        "outputId": "629c2e6f-c11c-41be-94a6-875813494091"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "# Load the state dictionary\n",
        "state_dict = torch.load(\"sapiens_0.3b_epoch_1600_clean.pth\")\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Zq0bGOY_zFs4",
        "outputId": "f65ed633-3773-46a9-da31-58f777ccccad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4097, 1024])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.4483,  0.1892,  0.3767,  ..., -0.2551, -0.3712,  0.3022],\n",
            "         [ 0.4364,  0.1943,  0.3770,  ..., -0.2410, -0.3435,  0.2985],\n",
            "         [ 0.4363,  0.1940,  0.3770,  ..., -0.2411, -0.3436,  0.2986],\n",
            "         ...,\n",
            "         [ 0.4379,  0.1944,  0.3791,  ..., -0.2385, -0.3422,  0.2984],\n",
            "         [ 0.4379,  0.1945,  0.3790,  ..., -0.2384, -0.3422,  0.2984],\n",
            "         [ 0.4378,  0.1945,  0.3789,  ..., -0.2383, -0.3421,  0.2984]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\"\n",
        "model.to(device)\n",
        "inputs = torch.ones(1, 3, 1024, 1024).to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "    print(outputs.shape)\n",
        "    print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gTymGUNfzKvB"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/facebook/sapiens-pretrain-0.3b-torchscript/resolve/main/sapiens_0.3b_epoch_1600_torchscript.pt2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the torchscript version of weights for efficient inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F1PeZ0D-zdcN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[[ 1.2733e-01,  4.7221e-02,  5.2698e-02,  ..., -1.0587e-01,\n",
              "            -1.0880e-01, -2.7345e-02],\n",
              "           [ 2.4000e-01,  1.5818e-01,  1.4922e-01,  ...,  1.4420e-02,\n",
              "             5.9127e-03,  5.7294e-02],\n",
              "           [ 3.3432e-01,  2.4099e-01,  2.2941e-01,  ...,  1.0003e-01,\n",
              "             9.6673e-02,  1.4691e-01],\n",
              "           ...,\n",
              "           [ 1.4163e-01,  3.4757e-02,  2.8229e-02,  ..., -3.0150e-02,\n",
              "            -3.9315e-02,  2.2697e-02],\n",
              "           [ 2.0953e-01,  1.0161e-01,  9.8985e-02,  ...,  2.3701e-02,\n",
              "             1.6074e-02,  7.9371e-02],\n",
              "           [ 3.8715e-01,  2.9326e-01,  2.7447e-01,  ...,  1.9486e-01,\n",
              "             1.8994e-01,  2.3577e-01]],\n",
              " \n",
              "          [[-8.6676e-02, -1.3280e-03,  4.7174e-02,  ...,  6.8181e-02,\n",
              "             1.6999e-02, -1.7683e-02],\n",
              "           [ 6.6773e-02,  1.2355e-01,  1.6407e-01,  ...,  2.0209e-01,\n",
              "             1.4679e-01,  1.4856e-01],\n",
              "           [ 1.3793e-01,  1.9407e-01,  2.3197e-01,  ...,  2.7402e-01,\n",
              "             2.2278e-01,  2.1753e-01],\n",
              "           ...,\n",
              "           [-7.5996e-02, -1.5113e-02,  2.6847e-02,  ...,  2.1239e-02,\n",
              "            -3.0313e-02, -3.6716e-02],\n",
              "           [-1.4533e-01, -7.3906e-02, -2.9830e-02,  ..., -3.6774e-02,\n",
              "            -9.3671e-02, -1.1128e-01],\n",
              "           [-1.5627e-01, -7.9662e-02, -4.0340e-02,  ..., -4.9572e-02,\n",
              "            -1.1056e-01, -1.5892e-01]],\n",
              " \n",
              "          [[-1.6310e-01, -1.2338e-01, -1.0593e-01,  ..., -1.4069e-01,\n",
              "            -1.5354e-01, -1.0301e-01],\n",
              "           [-1.2572e-01, -8.8959e-02, -7.2266e-02,  ..., -1.2314e-01,\n",
              "            -1.3349e-01, -7.3628e-02],\n",
              "           [-1.3054e-01, -8.8600e-02, -6.7747e-02,  ..., -1.1478e-01,\n",
              "            -1.2042e-01, -6.2442e-02],\n",
              "           ...,\n",
              "           [-1.7682e-01, -1.3371e-01, -1.1252e-01,  ..., -1.4154e-01,\n",
              "            -1.5246e-01, -8.4447e-02],\n",
              "           [-1.5796e-01, -1.2497e-01, -1.0105e-01,  ..., -1.2030e-01,\n",
              "            -1.3430e-01, -6.0775e-02],\n",
              "           [ 3.6816e-03,  1.8422e-02,  3.5947e-02,  ...,  4.2708e-02,\n",
              "             2.6791e-02,  7.2747e-02]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[-1.6137e-01, -1.4547e-01, -1.4369e-01,  ..., -9.9625e-02,\n",
              "            -8.7324e-02, -1.1097e-01],\n",
              "           [-1.2888e-01, -1.1152e-01, -1.0853e-01,  ..., -5.4374e-02,\n",
              "            -4.1310e-02, -7.6548e-02],\n",
              "           [-1.4288e-01, -1.2454e-01, -1.2210e-01,  ..., -6.6501e-02,\n",
              "            -5.0547e-02, -8.4969e-02],\n",
              "           ...,\n",
              "           [-9.7414e-02, -8.4733e-02, -8.1305e-02,  ..., -6.7060e-02,\n",
              "            -6.4024e-02, -9.8326e-02],\n",
              "           [-8.2023e-02, -7.2771e-02, -6.9782e-02,  ..., -4.8386e-02,\n",
              "            -4.5483e-02, -8.1355e-02],\n",
              "           [-4.6961e-02, -3.4951e-02, -3.2702e-02,  ..., -3.7132e-03,\n",
              "            -3.9459e-03, -5.1140e-02]],\n",
              " \n",
              "          [[-3.5708e-01, -2.8938e-01, -2.8080e-01,  ..., -2.7300e-01,\n",
              "            -2.6022e-01, -2.2502e-01],\n",
              "           [-3.4365e-01, -2.7756e-01, -2.6795e-01,  ..., -2.5557e-01,\n",
              "            -2.5086e-01, -2.1370e-01],\n",
              "           [-3.4238e-01, -2.8269e-01, -2.7185e-01,  ..., -2.6202e-01,\n",
              "            -2.6200e-01, -2.2070e-01],\n",
              "           ...,\n",
              "           [-2.8015e-01, -2.0705e-01, -1.9827e-01,  ..., -2.1790e-01,\n",
              "            -2.1263e-01, -1.8169e-01],\n",
              "           [-2.8620e-01, -2.1506e-01, -2.1001e-01,  ..., -2.3145e-01,\n",
              "            -2.2951e-01, -2.0026e-01],\n",
              "           [-3.9868e-01, -3.2379e-01, -3.1913e-01,  ..., -3.4379e-01,\n",
              "            -3.4001e-01, -3.0650e-01]],\n",
              " \n",
              "          [[-1.0446e-01, -7.5292e-02, -6.8466e-02,  ..., -3.8802e-02,\n",
              "            -3.6377e-02, -5.8167e-02],\n",
              "           [-8.1935e-02, -5.0875e-02, -4.3255e-02,  ..., -1.7184e-02,\n",
              "            -1.4363e-02, -3.4011e-02],\n",
              "           [-7.8366e-02, -4.8933e-02, -4.1361e-02,  ..., -1.5067e-02,\n",
              "            -1.1226e-02, -3.4482e-02],\n",
              "           ...,\n",
              "           [-3.1170e-02, -4.7811e-03, -4.8661e-04,  ...,  3.1171e-02,\n",
              "             3.1615e-02,  1.1055e-02],\n",
              "           [-1.9186e-02,  6.6525e-03,  9.6022e-03,  ...,  3.8646e-02,\n",
              "             4.0284e-02,  2.2150e-02],\n",
              "           [-2.2791e-02, -2.6296e-03, -8.1715e-05,  ...,  3.5553e-02,\n",
              "             4.0497e-02,  2.4146e-02]]]], device='cuda:0',\n",
              "        grad_fn=<PermuteBackward0>),)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "device = \"cuda\"\n",
        "torch.cuda.empty_cache()\n",
        "ts_model = torch.jit.load(\"sapiens_0.3b_epoch_1600_torchscript.pt2\")\n",
        "ts_model.to(device)\n",
        "inputs = torch.ones(1, 3, 1024, 1024).to(device)\n",
        "ts_output = ts_model(inputs)\n",
        "ts_output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
